# 《CUDA-C权威指南》学习笔记

GPU 居然可以有 上万个线程

> 本章通过研究成千上万的线程是如何在GPU中调度的，来探讨硬件 层面的内核执行问题。解释了计算资源是如何在多粒度线程间分配的，



GPU 性能的线性扩展。

> 还说明了在GPU加速计算集群上的 大规模应用是如何利用MPI与GPUDirect RDMA来实现性能线性扩展 的。



现代处理器都是 哈弗结构。

> 大多数现代处理器都应用了哈佛体系结构（Harvard architecture），如图1-1所示，它主要由3个部分组成



块划分 跟 周期划分

> 数据并行程序设计的第一步是把数据依据线程进行划分，以使每个 线程处理一部分数据。通常来说，有两种方法可以对数据进行划分：块 划分（block partitioning）和周期划分（cyclic partitioning）。在



数据是一维空间。

> 通常，数据是在一维空间中存储的。即便是多维逻辑数据，仍然要 被映射到一维物理地址空间中。如何在线程中分配数据不仅与数据的物 理储存方式密切相关，并且与每个线程的执行次序也有很大关系。组织 线程的方式对程序的性能有很大的影响。



计算机架构。

> ·单指令单数据（SISD） ·单指令多数据（SIMD） ·多指令单数据（MISD） ·多指令多数据（MIMD）



> 在CPU上编写代码时，程序员可以继续按 串行逻辑思考但对并行数据操作实现并行加速，而其他细节则由编译器 来负责。



单指令多线程。

> GPU代表了一种众核架构，几乎包括了前文描述的所有并行结构： 多线程、MIMD（多指令多数据）、SIMD（单指令多数据），以及指 令级并行。NVIDIA公司称这种架构为SIMT（单指令多线程）。



GPU 与 CPU 的历史。

> GPU和CPU的来源并不相同。历史上，GPU是图形加速器。直到最 近，GPU才演化成一个强大的、多用途的、完全可编程的，以及任务和 数据并行的处理器，它非常适合解决大规模的并行计算问题。



GPU 是协处理器。

> GPU不是一个独立运行的平台而是CPU的协处理器。



GPU 是硬件加速器

> 当使用CPU上的一个与其物理上分 离开的硬件组件来提高应用中的计算密集部分的执行速度时，这个组件 就成为了一个硬件加速器。GPU可以说是最为常见的硬件加速器。



> 以下产品应用了NVIDIA公司的GPU计算平台。 ·Tegra ·GeForce ·Quadro ·Tesla



![cuda-c-1-1](cuda-c-1-1.png)



> CPU上的线程通常是重量级的实体。操作系统必须交替线程使用启 用或关闭CPU执行通道以提供多线程处理功能。上下文的切换缓慢且开 销大。 GPU上的线程是高度轻量级的。在一个典型的系统中会有成千上万 的线程排队等待工作。如果GPU必须等待一组线程执行结束，那么它只 要调用另一组线程执行其他任务即可。 CPU的核被设计用来尽可能减少一个或两个线程运行时间的延迟， 而GPU的核是用来处理大量并发的、轻量级的线程，以最大限度地提高 吞吐量。



CUDA 通过扩展语言来实现接口。

> CUDA平台可以通过CUDA加速库、编译器指令、应用编程接口以 及行业标准程序语言的扩展（包括C、C++、Fortran、Python，如图1-12 所示）来使用。本书重点介绍CUDA C的编程。



CUDA 的汇编语言是 PTX 。



GPU 可以控制数据的局部性。

> 例如，在CUDA编程模型中使用的共享内存（一个特殊的内存）。 共享内存可以视为一个被软件管理的高速缓存，通过为主内存节省带宽 来大幅度提高运行速度。有了共享内存，你可以直接控制代码的数据局 部性。



CUDA 的抽象讨论。

> CUDA抽象了硬件细节，且不需要将应用程序映射到传统图形API 上。CUDA核中有3个关键抽象：线程组的层次结构，内存的层次结构 以及障碍同步。这3个抽象是最小的一组语言扩展。随着CUDA版本的 更新，NVIDIA正在对并行编程进行不断简化。尽管一些人仍然认为 CUDA的概念比较低级，但如果稍稍提高抽象级，对你控制应用程序和 平台之间的互动关系来说会增加很大难度。如果那样的话，不管你掌握 了多少底层架构的知识，你的应用程序的性能都将超出控制。



指定 C 语言标准 C99。

> nvcc -Xcompiler -std=c99 sumArraysOnHost.c –o sum



cudaMalloc 函数分配内存。

> cudaMalloc与标准C语言中的malloc函数几乎 一样，只是此函数在GPU的内存里分配内存
>



CUDA 相当于可以直接控制 L2 缓存。

> 全局类似于CPU的系统内存，而共享内存类似于CPU的缓存。然而 GPU的共享内存可以由CUDA C的内核直接控制。



CUDA 帮助文档。

> 你可以在CUDA编译 器文件中找到编译器选项（http://docs.nvidia.com/cuda/cuda-compilerdriver-nvcc/index.html）。



CUDA 6.0 的统一寻址功能。

> 为了避免这类错误，CUDA 6.0提出了统一寻址，使用一个指针来 访问CPU和GPU的内存



这是啥意思？

> 在CUDA程序中有两组不同的网格和块变量：手动定义的dim3数据 类型和预定义的uint3数据类型。



块变换，相应的网格也会变化。

> 下面是一个输出示例。由于应用程序中的数据大小是固定的，因此 当块的大小发生改变时，相应的网格尺寸也会发生改变。
>
> 要确定块尺寸，通常需要考虑： ·内核的性能特性 ·GPU资源的限制



CUDA 的线程模型。

> CUDA的特点之一就是通过编程模型揭示了一个两层的线程层次结 构。由于一个内核启动的网格和块的维数会影响性能，这一结构为程序 员优化程序提供了一个额外的途径。 网格和块的维度存在几个限制因素，对于块大小的一个主要限制因 素就是可利用的计算资源，如寄存器，共享内存等。某些限制可以通过 查询GPU设备撤回。



CUDA 核函数的语法。<<<>>> 里面是网格跟块参数，不是正常的参数。

> kernel_name<<<1,10>>>(argument list)

> 同一个块中的线程之间可以相互协作，不同块内的线程不能协作。 对于一个给定的问题，可以使用不同的网格和块布局来组织你的线程。



CUDA 的线程布局。

![cuda-c-1-2](D:\0-博客\study_log\《CUDA-C权威指南》\cuda-c-1-2.png)

数据在全局内存中是线性存储的

> 由于数据在全局内存中是线性存储的，因此可以用变量blockIdx.x 和threadId.x来进行以下操作。



`cudaDeviceSynchronize()` 函数可以让 CPU 阻塞等 GPU 完成任务。

>  核函数的调用与主机线程是异步的。核函数调用结束后，控制权立 刻返回给主机端。你可以调用以下函数来强制主机端程序等待所有的核 函数执行结束：



隐式同步。

> 一些CUDA运行时API在主机和设备之间是隐式同步的。当使用 cudaMemcpy函数在主机和设备之间拷贝数据时，主机端隐式同步，即 主机端程序必须等待数据拷贝完成后才能继续执行程序。



CUDA 核函数是异步的。

> 不同于C语言的函数调用，所有的CUDA核函数的启动都是异步 的。CUDA内核调用完成后，控制权立刻返回给CPU。



相同的计算，实际上就是一个大型的 SMID 机器。

> 当核函数被调用时，许多不同的 CUDA线程并行执行同一个计算任务。以下是用__global__声明定义核 函数：



核函数的返回值。

> 核函数必须有一个void返回类型。



函数类型。

![cuda-c-1-3](D:\0-博客\study_log\《CUDA-C权威指南》\cuda-c-1-3.png)

核函数的限制：

> 以下限制适用于所有核函数： ·只能访问设备内存 ·必须具有void返回类型 ·不支持可变数量的参数 ·不支持静态变量 ·显示异步行为



N的含义。

> 由于N是被隐式定义用来启 动N个线程的，所以N没有什么参考价值。



可以模拟串行执行。

> 可以将执行参数设置为<<<1，1>>>，因此强制用一个块和 一个线程执行核函数，这模拟了串行执行程序



CHECK 宏的作用。

> CHECK（cudaDeviceSynchronize（））



结构

![1-2-2](D:\0-博客\study_log\《CUDA-C权威指南》\cuda-c-1-2-2.png)

注意他这个块的计算。

> 如果你将执行配置重新定义为32个块，每个块只有一个元素，如下 所示：



正确的计算索引的方式。

![cuda-c-1-3-4](D:\0-博客\study_log\《CUDA-C权威指南》\cuda-c-1-3-4.png)

网格的限制

> CUDA提供了通过查询GPU来了解这些限制的能力。在本章的2.4节 有详细的介绍。 对于Fermi设备，每个块的最大线程数是1024，且网格的x、y、z三 个方向上的维度最大值是65535。



nvprof 性能分析工具。

> 自CUDA 5.0以来，NVIDIA提供了一个名为nvprof的命令行分析工 具，可以帮助从应用程序的CPU和GPU活动情况中获取时间线信息，其 包括内核执行、内存传输以及CUDA API的调用。其用法如下。



数据传输与计算的成本比较。

> 对于HPC工作负载，理解程序中通信比的计算是非常重要的。如果 你的应用程序用于计算的时间大于数据传输所用的时间，那么或许可以 压缩这些操作，并完全隐藏与传输数据有关的延迟。如果你的应用程序 用于计算的时间少于数据传输所用的时间，那么需要尽量减少主机和设 备之间的传输。



性能。

> ·传统的核函数实现一般不能获得最佳性能 ·对于一个给定的核函数，尝试使用不同的网格和线程块大小可以 获得更好的性能



注意这个命令行 nvidia-smi

> NVIDIA系统管理界面（nvidia-smi）命令行实用程序



查询 GPU 信息的 API 函数。

> 在CUDA运行时API中有很多函数可以帮助管理这些设备。可以使 用以下函数查询关于GPU设备的所



环境变量 CUDA_VISIBLE_DEVICES

> 设置运行时环境变量CUDA_VISIBLE_DEVICES=2。nvidia驱动程 序会屏蔽其他GPU，这时设备2作为设备0出现在应用程序中。



SM

> GPU架构是围绕一个流式多处理器（SM）的可扩展阵列搭建的。 可以通过复制这种架构的构建块来实现GPU的硬件并行。



CUDA采用单指令多线程（SIMT）架构来管理和执行线程

> CUDA采用单指令多线程（SIMT）架构来管理和执行线程，每32 个线程为一组，被称为线程束（warp）。线程束中的所有线程同时执行 相同的指令。每个线程都有自己的指令地址计数器和寄存器状态，利用 自身的数据执行当前的指令。每个SM都将分配给它的线程块划分到包 含32个线程的线程束中，然后在可用的硬件资源上调度执行。



SIMT 与 SIMD 是类似的。

> SIMT架构与SIMD（单指令多数据）架构相似。两者都是将相同的 指令广播给多个执行单元来实现并行。一个关键的区别是SIMD要求同 一个向量中的所有元素要在一个统一的同步组中一起执行，而SIMT允 许属于同一线程束的多个线程独立执行。尽管一个线程束中的所有线程 在相同的程序地址上同时开始执行，但是单独的线程仍有可能有不同的 行为。SIMT确保可以编写独立的线程级并行代码、标量线程以及用于 协调线程的数据并行代码。



> 从概念上讲，它是SM用SIMD方式所同时处理的工作粒度。优化工 作负载以适应线程束（一组有32个线程）的边界，一般这样会更有效地 利用GPU计算资源。在后面的章节中将会介绍更多这方面的内容。



> 尽管线程块里的所有线程都可以逻辑地并行运行，但是并不是所有 线程都可以同时在物理层面执行。因此，线程块里的不同线程可能会以 不同的速度前进



> CUDA提供了一种 用来同步线程块里的线程的方法，从而保证所有线程在进一步动作之前 都达到执行过程中的一个特定点。



MIMD 架构。

> 从程序员的角度看，并发内核执行使GPU表现得更像 MIMD架构。



动态并行

> 动态并行是Kepler GPU的一个新特性，它允许GPU动态启动新的网 格。



Hyper-Q技术 的优势。

> Hyper-Q技术增加了更多的CPU和GPU之间的同步硬件连接，以确 保CPU核心能够在GPU上同时运行更多的任务。因此，当使用Kepler GPU时，既可以增加GPU的利用率，也可以减少CPU的闲置时间。 Fermi GPU依赖一个单一的硬件工作队列来从CPU到GPU间传送任务， 这可能会导致一个单独的任务阻塞队列中在该任务之后的所有其他任 务。Kepler Hyper-Q消除了这个限制。如图3-9所示，Kepler GPU在主机 与GPU之间提供了32个硬件工作队列。Hyper-Q保证了在GPU上有更多 的并发执行，最大限度地提高了GPU的利用并提高了整体的性能。



性能分析工具，nvvp 跟 nvprof

> CUDA提供了两个主要的性能分析工具：nvvp，独立的可视化分析 器；nvprof，命令行分析器。



注意 Nsight Eclipse Edition

> 该工具既可作为一个独立的应用程序，也可作 为Nsight Eclipse Edition（nsight）的一部分



分支预测。

> CPU拥有复杂的硬件以执行分支预测，也就是在每个条件检查中预 测应用程序的控制流会使用哪个分支。如果预测正确，CPU中的分支只 需付出很小的性能代价。如果预测不正确，CPU可能会停止运行很多个 周期，因为指令流水线被清空了。我们不必完全理解为什么CPU擅长处 理复杂的控制流。这个解释只是作为对比的背景。 GPU是相对简单的设备，它没有复杂的分支预测机制。一个线程束 中的所有线程在同一周期中必须执行相同的指令，如果一个线程执行一 条指令，那么线程束中的所有线程都必须执行该指令。如果在同一线程 束中的线程使用不同的路径通过同一个应用程序，这可能会产生问题。 例如，思考下面的语句：



注意线程束分化。

> 而另一半需要执行else语句块中的指令。在同一线程束中的线程执 行不同的指令，被称为线程束分化。我们已经知道，在一个线程束中所 有线程在每个周期中必须执行相同的指令，所以线程束分化似乎会产生 一个悖论。



> 使用这些准则可以使应用程序适用于当前和将来的设备：
>
> 保持每个块中线程数量是线程束大小（32）的倍数 ·避免块太小：每个块至少要有128或256个线程 ·根据内核资源的需求调整块大小 ·块的数量要远远多于SM的数量，从而在设备中可以显示有足够的 并行 ·通过实验得到最佳执行配置和资源使用情况



同步

> 对于主机来说，由于许多CUDA API调用和所有的内核启动不是同 步的，cudaDeviceSyn-chronize函数可以用来阻塞主机应用程序，直到所 有的CUDA操作（复制、核函数等）完成：



注意一下 并行归约问题 

> 如果有大量的数据元素会怎么样呢？如何通过并行计算快速求和 呢？鉴于加法的结合律和交换律，数组元素可以以任何顺序求和。所以 可以用以下的方法执行并行加法运算： 1.将输入向量划分到更小的数据块中。 2.用一个线程计算一个数据块的部分和。 3.对每个数据块的部分和再求和得出最终结果。



交错实现性能优化。

> 交错实现比第一个实现快了1.69倍，比第二个实现快了1.34倍。这 种性能的提升主要是由reduceInterleaved函数里的全局内存加载/存储模 式导致的。



循环展开 的优化方式

> 循环展开是一个尝试通过减少分支出现的频率和循环维护指令来优 化循环的技术。在循环展开中，循环主体在代码中要多次被编写，而不 是只编写一次循环主体再使用另一个循环来反复执行的。任何的封闭循 环可将它的迭代次数减少或完全删除。循环体的复制数量被称为循环展 开因子，迭代次数就变为了原始循环迭代次数除以循环展开因子。在顺 序数组中，当循环的迭代次数在循环执行之前就已经知道时，循环展开 是最有效提升性能的方法。考虑下面的代码：





> 在CUDA中，循环展开的意义非常重大。我们的目标仍然是相同 的：通过减少指令消耗和增加更多的独立调度指令来提高性能。因此， 更多的并发操作被添加到流水线上，以产生更高的指令和内存带宽。这 为线程束调度器提供更多符合条件的线程束，它们可以帮助隐藏指令或 内存延迟



模板函数展开规约。

> 虽然可以手动展开循环，但是使用模板函数有助于进一步减少分支 消耗。在设备函数上CUDA支持模板参数。如下所示，可以指定块的大 小作为模板函数的参数：













