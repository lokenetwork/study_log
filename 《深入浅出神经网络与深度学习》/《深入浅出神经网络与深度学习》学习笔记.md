# 《深入浅出神经网络与深度学习》学习笔记

技术兴衰起落，而原理是长久的

> 这意味着本书不会重点讲授如何使用特定的神经网络库。如果你想学习某个程序库的用法， 可以参考相关教程和文档。请注意，这样可能暂时解决了某些问题，然而如果想理解神经网络的 运行机制，以及未来几年都不会过时的原理，那么只学习流行的程序库是不够的，还需要掌握神 经网络的工作原理。技术兴衰起落，而原理是长久的。



如果试着细化识别规则以提高准确度， 很快就会出现各种异常和特殊的情形，似乎毫无希望

> 如果尝试编写计算机程序来识别以上数字，就会发现视觉模式识别的复杂性，人类可以轻松 完成的任务顿时变得困难重重。识别形状时，对于“数字 9 的上半部分是一个圈，右下部分是一 条竖线”这样的简单直觉，实际上很难用算法表达出来。如果试着细化识别规则以提高准确度， 很快就会出现各种异常和特殊的情形，似乎毫无希望。



神经网络正是通过这种方式进行学习的。

> 如果确实如此，那么可以通过修改权重和偏置来调整网络表现。假设神经网络错误地把一个 “9”的图像分类为了“8”，我们可以计算如何修改权重和偏置，以使神经网络能够把图像分类为 “9”。然后重复这项工作，反复改动权重和偏置来产生更好的输出。神经网络正是通过这种方式 进行学习的。



sigmoid 神经元是小数来的。

> 类似于感知机，sigmoid 神经元有多个输入 1 2 x x, ,，但是这些输入可以取 0 到 1 的任意值， 而不限于 0 或 1，例如 0.638...就是 sigmoid 神经元的一个有效输入。同样，sigmoid 神经元对每个 输入有权重 1 2 w w, ,和一个总的偏置 b，但是输出不是 0 或 1，而是( ) w x b ，其中的 被称



术语区别

> 令人困惑的是，由于历史的原因，尽管这种多层神经网络是由 sigmoid 神经元而不是感知机 构成的，但有时仍被称为多层感知机（multilayer perceptron，MLP），但本书不会使用这种称法， 因为可能引起混淆，这里只做简单介绍。



前馈神经网络 与 循环神经网络

> 前面介绍的神经网络都以上一层的输出作为下一层的输入，这种神经网络叫作前馈神经网络。 这意味着神经网络中是没有回路的，即信息总是向前传播，从不反向回馈。如果存在回路，最终 会出现这样的状况：sigmoid 函数的输入依赖输出。这难以理解，所以不允许出现回路。 
>
> 然而，一些人工神经网络模型可以包含反馈回路，这类模型被称为循环神经网络。其设计思 想是部分神经元在休眠前会保持激活状态，这种激活状态可以刺激其他神经元，将其激活并保持 一段时间，这样会导致更多神经元被激活。随着时间的推移，就会得到一个级联的神经元激活系 统。因为一个神经元的输出在一段时间后而不是即刻影响其输入，所以在该模型中回路不会引起 问题。



简单的智能

> 当然，科学史上有很多这样的例子。例如构成整个世界的神秘的化学物质，被门捷列夫的元 素周期表巧妙地解释了，而这些规律又能被更简单的量子力学规律解释。又如生物界中太多的复 杂性和多样性产生了诸多困扰，最终发现答案是自然选择作用的演化规律。这些例子都表明，如 果仅仅由于人类大脑（目前人类大脑是智能的最佳体现）表现出复杂的功能便断定关于智能的简 单解释不成立，那么这其实是不明智的做法①。



大脑的不同部分可能以共同的 原理进行学习

> 这相当令人吃惊。它表明大脑的不同部分存在共同的原理去学习对数据做出反应。这种共性 支持了“简单的原理能产生智能”这一猜想，但这点成果无法确保万事大吉。行为测试仅仅对视 觉进行了粗略的验证，而且我们也没法问雪貂它们是否“学会看”了。因此实验并没有证明重连 的听觉皮层能给予雪貂精确的视觉体验，而只是用有限的证据表明大脑的不同部分可能以共同的 原理进行学习。









